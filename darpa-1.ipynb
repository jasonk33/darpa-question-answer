{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 255 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, bigrams, ngrams\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder, scale\n",
    "from nltk.corpus import stopwords\n",
    "import us\n",
    "import pycountry\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "question_data = pd.read_csv('darpa_problems/r_32/data/raw_data/questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 97.5 ms\n"
     ]
    }
   ],
   "source": [
    "sentence_data = pd.read_csv('darpa_problems/r_32/data/raw_data/sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38.9 ms\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('darpa_problems/r_32/data/trainData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.5 ms\n"
     ]
    }
   ],
   "source": [
    "train_targets = pd.read_csv('darpa_problems/r_32/data/trainTargets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.47 ms\n"
     ]
    }
   ],
   "source": [
    "def mynonrandom():\n",
    " return 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2, 4, 1, 7]"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.15 ms\n"
     ]
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.4 ms\n"
     ]
    }
   ],
   "source": [
    "def remove_data(data, perc_no):\n",
    "    num_samples = len(data['response'])\n",
    "    num_yes = num_samples*np.mean(data['response'])\n",
    "    num_no = int(num_yes/(1-perc_no) - num_yes)\n",
    "    yes_idxs = [idx for idx, response in enumerate(data['response']) if response == 1]\n",
    "    no_idxs = [idx for idx, response in enumerate(data['response']) if response == 0]\n",
    "    random.shuffle(no_idxs, mynonrandom)\n",
    "    new_no_idxs = no_idxs[0:int(num_no)]\n",
    "    new_idxs = yes_idxs + new_no_idxs\n",
    "    new_data = {}\n",
    "    for key, value in data.items():\n",
    "        new_data[key] = [value[idx] for idx in new_idxs]\n",
    "    return [new_data, num_yes+num_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/JasonKatz/Desktop/darpa_problems/r_32/data/data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 260 ms\n"
     ]
    }
   ],
   "source": [
    "def model_with_subset(data, max_perc=.95, model=LogisticRegression, iters = 5):\n",
    "    new_data, num_samples = remove_data(data, max_perc)\n",
    "    encoder = LabelBinarizer()\n",
    "    input_variables = encoder.fit_transform(new_data['first_word'])\n",
    "    for variable in ['matches', 'last_word', 'length', 'number', 'place', 'what_year', 'what_is', 'is', 'how_many']:\n",
    "        input_variables = np.hstack((input_variables, np.array(new_data[variable])[:, np.newaxis]))\n",
    "    accuracies = 0\n",
    "    naive = 0\n",
    "    confusion_mat = np.zeros((2,2))\n",
    "    for state in range(iters):\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(input_variables, new_data['response'], random_state=state)\n",
    "        clf = model()\n",
    "        clf.fit(xtrain, ytrain)\n",
    "        ypred_test = clf.predict(xtest)\n",
    "        accuracies += accuracy_score(ytest, ypred_test)\n",
    "        naive += 1-np.mean(ytest)\n",
    "        confusion_mat += confusion_matrix(ytest, ypred_test)\n",
    "    model_accuracy = accuracies/iters\n",
    "    naive_accuracy = naive/iters\n",
    "    confusion_mat = (confusion_mat/iters).astype(int)\n",
    "    model_total = 0\n",
    "    naive_total = 0\n",
    "    for state in range(iters):\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(input_variables, new_data['response'], random_state=state)\n",
    "        ave_yes = np.mean(ytest)\n",
    "        clf = model()\n",
    "        clf.fit(xtrain, ytrain)\n",
    "        ypred_test = clf.predict_proba(xtest)\n",
    "        single_naive = np.array([[1-ave_yes, ave_yes], [1-ave_yes, ave_yes]])\n",
    "        naive_predictions = np.repeat(single_naive, [0, len(ytest)], axis=0)\n",
    "        model_total += log_loss(ytest, ypred_test)\n",
    "        naive_total += log_loss(ytest, naive_predictions)\n",
    "    model_log_loss = model_total/iters\n",
    "    naive_log_loss = naive_total/iters\n",
    "    perc_no = 1 - np.mean(new_data['response'])\n",
    "    return pd.DataFrame([[model.__name__, num_samples, perc_no, model_accuracy, naive_accuracy, model_log_loss, \n",
    "                          naive_log_loss, confusion_mat]], columns=['Model', 'Number_Samples', 'Percent_No', \n",
    "                                                                           'Model_Accuracy', 'Naive_Accuracy', \n",
    "                                                                           'Model_Log_Loss', 'Naive_Log_Loss', \n",
    "                                                                           'Confusion_Matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Number_Samples</th>\n",
       "      <th>Percent_No</th>\n",
       "      <th>Model_Accuracy</th>\n",
       "      <th>Naive_Accuracy</th>\n",
       "      <th>Model_Log_Loss</th>\n",
       "      <th>Naive_Log_Loss</th>\n",
       "      <th>Confusion_Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>23599.0</td>\n",
       "      <td>0.948902</td>\n",
       "      <td>0.946623</td>\n",
       "      <td>0.946761</td>\n",
       "      <td>0.186489</td>\n",
       "      <td>0.207929</td>\n",
       "      <td>[[5464, 2], [306, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>23599.0</td>\n",
       "      <td>0.948902</td>\n",
       "      <td>0.935123</td>\n",
       "      <td>0.946761</td>\n",
       "      <td>0.288796</td>\n",
       "      <td>0.207929</td>\n",
       "      <td>[[5373, 93], [281, 26]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>23599.0</td>\n",
       "      <td>0.948902</td>\n",
       "      <td>0.931001</td>\n",
       "      <td>0.946761</td>\n",
       "      <td>1.113961</td>\n",
       "      <td>0.207929</td>\n",
       "      <td>[[5354, 112], [285, 21]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>23599.0</td>\n",
       "      <td>0.948902</td>\n",
       "      <td>0.944475</td>\n",
       "      <td>0.946761</td>\n",
       "      <td>1.232065</td>\n",
       "      <td>0.207929</td>\n",
       "      <td>[[5446, 20], [300, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>23599.0</td>\n",
       "      <td>0.948902</td>\n",
       "      <td>0.946727</td>\n",
       "      <td>0.946761</td>\n",
       "      <td>0.187652</td>\n",
       "      <td>0.207929</td>\n",
       "      <td>[[5465, 1], [306, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>7866.0</td>\n",
       "      <td>0.849987</td>\n",
       "      <td>0.855211</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>0.365906</td>\n",
       "      <td>0.420039</td>\n",
       "      <td>[[1650, 24], [260, 31]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>7866.0</td>\n",
       "      <td>0.849987</td>\n",
       "      <td>0.837417</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>0.572781</td>\n",
       "      <td>0.420039</td>\n",
       "      <td>[[1579, 95], [224, 67]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>7866.0</td>\n",
       "      <td>0.849987</td>\n",
       "      <td>0.823386</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>1.871334</td>\n",
       "      <td>0.420039</td>\n",
       "      <td>[[1554, 120], [227, 64]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>7866.0</td>\n",
       "      <td>0.849987</td>\n",
       "      <td>0.837824</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>2.083036</td>\n",
       "      <td>0.420039</td>\n",
       "      <td>[[1606, 68], [250, 41]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>7866.0</td>\n",
       "      <td>0.849987</td>\n",
       "      <td>0.854906</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>0.367330</td>\n",
       "      <td>0.420039</td>\n",
       "      <td>[[1645, 29], [256, 36]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.779492</td>\n",
       "      <td>0.743051</td>\n",
       "      <td>0.482472</td>\n",
       "      <td>0.569562</td>\n",
       "      <td>[[828, 48], [212, 91]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.745932</td>\n",
       "      <td>0.743051</td>\n",
       "      <td>1.102248</td>\n",
       "      <td>0.569562</td>\n",
       "      <td>[[840, 36], [263, 40]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.735424</td>\n",
       "      <td>0.743051</td>\n",
       "      <td>2.160667</td>\n",
       "      <td>0.569562</td>\n",
       "      <td>[[758, 118], [193, 109]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.747119</td>\n",
       "      <td>0.743051</td>\n",
       "      <td>2.293382</td>\n",
       "      <td>0.569562</td>\n",
       "      <td>[[786, 90], [208, 94]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.777966</td>\n",
       "      <td>0.743051</td>\n",
       "      <td>0.480584</td>\n",
       "      <td>0.569562</td>\n",
       "      <td>[[841, 35], [226, 76]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>3371.0</td>\n",
       "      <td>0.649956</td>\n",
       "      <td>0.749466</td>\n",
       "      <td>0.646738</td>\n",
       "      <td>0.521318</td>\n",
       "      <td>0.648699</td>\n",
       "      <td>[[472, 72], [138, 159]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>3371.0</td>\n",
       "      <td>0.649956</td>\n",
       "      <td>0.683037</td>\n",
       "      <td>0.646738</td>\n",
       "      <td>1.240302</td>\n",
       "      <td>0.648699</td>\n",
       "      <td>[[493, 51], [215, 82]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>3371.0</td>\n",
       "      <td>0.649956</td>\n",
       "      <td>0.699644</td>\n",
       "      <td>0.646738</td>\n",
       "      <td>2.115990</td>\n",
       "      <td>0.648699</td>\n",
       "      <td>[[437, 108], [145, 152]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>3371.0</td>\n",
       "      <td>0.649956</td>\n",
       "      <td>0.693476</td>\n",
       "      <td>0.646738</td>\n",
       "      <td>2.211720</td>\n",
       "      <td>0.648699</td>\n",
       "      <td>[[447, 97], [160, 137]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>3371.0</td>\n",
       "      <td>0.649956</td>\n",
       "      <td>0.751127</td>\n",
       "      <td>0.646738</td>\n",
       "      <td>0.520116</td>\n",
       "      <td>0.648699</td>\n",
       "      <td>[[474, 71], [138, 159]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>2622.0</td>\n",
       "      <td>0.549962</td>\n",
       "      <td>0.717378</td>\n",
       "      <td>0.556402</td>\n",
       "      <td>0.555129</td>\n",
       "      <td>0.686698</td>\n",
       "      <td>[[279, 85], [99, 191]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>2622.0</td>\n",
       "      <td>0.549962</td>\n",
       "      <td>0.635061</td>\n",
       "      <td>0.556402</td>\n",
       "      <td>1.528431</td>\n",
       "      <td>0.686698</td>\n",
       "      <td>[[331, 33], [206, 84]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>2622.0</td>\n",
       "      <td>0.549962</td>\n",
       "      <td>0.651524</td>\n",
       "      <td>0.556402</td>\n",
       "      <td>2.031769</td>\n",
       "      <td>0.686698</td>\n",
       "      <td>[[257, 108], [120, 170]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>2622.0</td>\n",
       "      <td>0.549962</td>\n",
       "      <td>0.647561</td>\n",
       "      <td>0.556402</td>\n",
       "      <td>2.186162</td>\n",
       "      <td>0.686698</td>\n",
       "      <td>[[261, 103], [128, 163]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>2622.0</td>\n",
       "      <td>0.549962</td>\n",
       "      <td>0.716768</td>\n",
       "      <td>0.556402</td>\n",
       "      <td>0.555837</td>\n",
       "      <td>0.686698</td>\n",
       "      <td>[[280, 85], [100, 190]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.711525</td>\n",
       "      <td>0.494576</td>\n",
       "      <td>0.569961</td>\n",
       "      <td>0.692644</td>\n",
       "      <td>[[205, 86], [83, 214]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.567458</td>\n",
       "      <td>0.494576</td>\n",
       "      <td>1.818654</td>\n",
       "      <td>0.692644</td>\n",
       "      <td>[[270, 21], [234, 64]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.633220</td>\n",
       "      <td>0.494576</td>\n",
       "      <td>2.247721</td>\n",
       "      <td>0.692644</td>\n",
       "      <td>[[189, 102], [113, 184]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.635932</td>\n",
       "      <td>0.494576</td>\n",
       "      <td>2.313092</td>\n",
       "      <td>0.692644</td>\n",
       "      <td>[[190, 101], [113, 185]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.494576</td>\n",
       "      <td>0.570580</td>\n",
       "      <td>0.692644</td>\n",
       "      <td>[[194, 97], [72, 225]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Number_Samples  Percent_No  Model_Accuracy  \\\n",
       "0      LogisticRegression         23599.0    0.948902        0.946623   \n",
       "0              GaussianNB         23599.0    0.948902        0.935123   \n",
       "0  RandomForestClassifier         23599.0    0.948902        0.931001   \n",
       "0    KNeighborsClassifier         23599.0    0.948902        0.944475   \n",
       "0           MLPClassifier         23599.0    0.948902        0.946727   \n",
       "0      LogisticRegression          7866.0    0.849987        0.855211   \n",
       "0              GaussianNB          7866.0    0.849987        0.837417   \n",
       "0  RandomForestClassifier          7866.0    0.849987        0.823386   \n",
       "0    KNeighborsClassifier          7866.0    0.849987        0.837824   \n",
       "0           MLPClassifier          7866.0    0.849987        0.854906   \n",
       "0      LogisticRegression          4720.0    0.750000        0.779492   \n",
       "0              GaussianNB          4720.0    0.750000        0.745932   \n",
       "0  RandomForestClassifier          4720.0    0.750000        0.735424   \n",
       "0    KNeighborsClassifier          4720.0    0.750000        0.747119   \n",
       "0           MLPClassifier          4720.0    0.750000        0.777966   \n",
       "0      LogisticRegression          3371.0    0.649956        0.749466   \n",
       "0              GaussianNB          3371.0    0.649956        0.683037   \n",
       "0  RandomForestClassifier          3371.0    0.649956        0.699644   \n",
       "0    KNeighborsClassifier          3371.0    0.649956        0.693476   \n",
       "0           MLPClassifier          3371.0    0.649956        0.751127   \n",
       "0      LogisticRegression          2622.0    0.549962        0.717378   \n",
       "0              GaussianNB          2622.0    0.549962        0.635061   \n",
       "0  RandomForestClassifier          2622.0    0.549962        0.651524   \n",
       "0    KNeighborsClassifier          2622.0    0.549962        0.647561   \n",
       "0           MLPClassifier          2622.0    0.549962        0.716768   \n",
       "0      LogisticRegression          2360.0    0.500000        0.711525   \n",
       "0              GaussianNB          2360.0    0.500000        0.567458   \n",
       "0  RandomForestClassifier          2360.0    0.500000        0.633220   \n",
       "0    KNeighborsClassifier          2360.0    0.500000        0.635932   \n",
       "0           MLPClassifier          2360.0    0.500000        0.711864   \n",
       "\n",
       "   Naive_Accuracy  Model_Log_Loss  Naive_Log_Loss          Confusion_Matrix  \n",
       "0        0.946761        0.186489        0.207929     [[5464, 2], [306, 1]]  \n",
       "0        0.946761        0.288796        0.207929   [[5373, 93], [281, 26]]  \n",
       "0        0.946761        1.113961        0.207929  [[5354, 112], [285, 21]]  \n",
       "0        0.946761        1.232065        0.207929    [[5446, 20], [300, 7]]  \n",
       "0        0.946761        0.187652        0.207929     [[5465, 1], [306, 0]]  \n",
       "0        0.851449        0.365906        0.420039   [[1650, 24], [260, 31]]  \n",
       "0        0.851449        0.572781        0.420039   [[1579, 95], [224, 67]]  \n",
       "0        0.851449        1.871334        0.420039  [[1554, 120], [227, 64]]  \n",
       "0        0.851449        2.083036        0.420039   [[1606, 68], [250, 41]]  \n",
       "0        0.851449        0.367330        0.420039   [[1645, 29], [256, 36]]  \n",
       "0        0.743051        0.482472        0.569562    [[828, 48], [212, 91]]  \n",
       "0        0.743051        1.102248        0.569562    [[840, 36], [263, 40]]  \n",
       "0        0.743051        2.160667        0.569562  [[758, 118], [193, 109]]  \n",
       "0        0.743051        2.293382        0.569562    [[786, 90], [208, 94]]  \n",
       "0        0.743051        0.480584        0.569562    [[841, 35], [226, 76]]  \n",
       "0        0.646738        0.521318        0.648699   [[472, 72], [138, 159]]  \n",
       "0        0.646738        1.240302        0.648699    [[493, 51], [215, 82]]  \n",
       "0        0.646738        2.115990        0.648699  [[437, 108], [145, 152]]  \n",
       "0        0.646738        2.211720        0.648699   [[447, 97], [160, 137]]  \n",
       "0        0.646738        0.520116        0.648699   [[474, 71], [138, 159]]  \n",
       "0        0.556402        0.555129        0.686698    [[279, 85], [99, 191]]  \n",
       "0        0.556402        1.528431        0.686698    [[331, 33], [206, 84]]  \n",
       "0        0.556402        2.031769        0.686698  [[257, 108], [120, 170]]  \n",
       "0        0.556402        2.186162        0.686698  [[261, 103], [128, 163]]  \n",
       "0        0.556402        0.555837        0.686698   [[280, 85], [100, 190]]  \n",
       "0        0.494576        0.569961        0.692644    [[205, 86], [83, 214]]  \n",
       "0        0.494576        1.818654        0.692644    [[270, 21], [234, 64]]  \n",
       "0        0.494576        2.247721        0.692644  [[189, 102], [113, 184]]  \n",
       "0        0.494576        2.313092        0.692644  [[190, 101], [113, 185]]  \n",
       "0        0.494576        0.570580        0.692644    [[194, 97], [72, 225]]  "
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "model_table = pd.DataFrame(columns=['Model', 'Number_Samples', 'Percent_No', 'Model_Accuracy', 'Naive_Accuracy', \n",
    "                                    'Model_Log_Loss', 'Naive_Log_Loss', 'Confusion_Matrix'])\n",
    "for perc_no in [.95, .85, .75, .65, .55, .5]:\n",
    "    for model in [LogisticRegression, GaussianNB, RandomForestClassifier, KNeighborsClassifier, MLPClassifier]:\n",
    "        model_table = model_table.append(model_with_subset(data, perc_no, model))\n",
    "model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mean Model Accuracy: 76.35%, Mean Naive Accuracy: 70.65%'"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.1 ms\n"
     ]
    }
   ],
   "source": [
    "\"Mean Model Accuracy: {}%, Mean Naive Accuracy: {}%\".format(100*round(np.mean(model_table['Model_Accuracy']),4), \n",
    "                                                            100*round(np.mean(model_table['Naive_Accuracy']),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number_Samples</th>\n",
       "      <th>Model_Accuracy</th>\n",
       "      <th>Naive_Accuracy</th>\n",
       "      <th>Model_Log_Loss</th>\n",
       "      <th>Naive_Log_Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percent_No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.500000</th>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>0.494576</td>\n",
       "      <td>1.504002</td>\n",
       "      <td>0.692644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.549962</th>\n",
       "      <td>2622.0</td>\n",
       "      <td>0.673659</td>\n",
       "      <td>0.556402</td>\n",
       "      <td>1.371466</td>\n",
       "      <td>0.686698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.649956</th>\n",
       "      <td>3371.0</td>\n",
       "      <td>0.715350</td>\n",
       "      <td>0.646738</td>\n",
       "      <td>1.321889</td>\n",
       "      <td>0.648699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.750000</th>\n",
       "      <td>4720.0</td>\n",
       "      <td>0.757186</td>\n",
       "      <td>0.743051</td>\n",
       "      <td>1.303871</td>\n",
       "      <td>0.569562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.849987</th>\n",
       "      <td>7866.0</td>\n",
       "      <td>0.841749</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>1.052077</td>\n",
       "      <td>0.420039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.948902</th>\n",
       "      <td>23599.0</td>\n",
       "      <td>0.940790</td>\n",
       "      <td>0.946761</td>\n",
       "      <td>0.601793</td>\n",
       "      <td>0.207929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Number_Samples  Model_Accuracy  Naive_Accuracy  Model_Log_Loss  \\\n",
       "Percent_No                                                                   \n",
       "0.500000            2360.0        0.652000        0.494576        1.504002   \n",
       "0.549962            2622.0        0.673659        0.556402        1.371466   \n",
       "0.649956            3371.0        0.715350        0.646738        1.321889   \n",
       "0.750000            4720.0        0.757186        0.743051        1.303871   \n",
       "0.849987            7866.0        0.841749        0.851449        1.052077   \n",
       "0.948902           23599.0        0.940790        0.946761        0.601793   \n",
       "\n",
       "            Naive_Log_Loss  \n",
       "Percent_No                  \n",
       "0.500000          0.692644  \n",
       "0.549962          0.686698  \n",
       "0.649956          0.648699  \n",
       "0.750000          0.569562  \n",
       "0.849987          0.420039  \n",
       "0.948902          0.207929  "
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 192 ms\n"
     ]
    }
   ],
   "source": [
    "model_table.groupby('Percent_No').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number_Samples</th>\n",
       "      <th>Percent_No</th>\n",
       "      <th>Model_Accuracy</th>\n",
       "      <th>Naive_Accuracy</th>\n",
       "      <th>Model_Log_Loss</th>\n",
       "      <th>Naive_Log_Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>7423.0</td>\n",
       "      <td>0.708134</td>\n",
       "      <td>0.734005</td>\n",
       "      <td>0.706496</td>\n",
       "      <td>1.091869</td>\n",
       "      <td>0.537595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>7423.0</td>\n",
       "      <td>0.708134</td>\n",
       "      <td>0.751064</td>\n",
       "      <td>0.706496</td>\n",
       "      <td>2.053243</td>\n",
       "      <td>0.537595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>7423.0</td>\n",
       "      <td>0.708134</td>\n",
       "      <td>0.793282</td>\n",
       "      <td>0.706496</td>\n",
       "      <td>0.446879</td>\n",
       "      <td>0.537595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>7423.0</td>\n",
       "      <td>0.708134</td>\n",
       "      <td>0.793226</td>\n",
       "      <td>0.706496</td>\n",
       "      <td>0.447017</td>\n",
       "      <td>0.537595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>7423.0</td>\n",
       "      <td>0.708134</td>\n",
       "      <td>0.745700</td>\n",
       "      <td>0.706496</td>\n",
       "      <td>1.923574</td>\n",
       "      <td>0.537595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Number_Samples  Percent_No  Model_Accuracy  \\\n",
       "Model                                                                \n",
       "GaussianNB                      7423.0    0.708134        0.734005   \n",
       "KNeighborsClassifier            7423.0    0.708134        0.751064   \n",
       "LogisticRegression              7423.0    0.708134        0.793282   \n",
       "MLPClassifier                   7423.0    0.708134        0.793226   \n",
       "RandomForestClassifier          7423.0    0.708134        0.745700   \n",
       "\n",
       "                        Naive_Accuracy  Model_Log_Loss  Naive_Log_Loss  \n",
       "Model                                                                   \n",
       "GaussianNB                    0.706496        1.091869        0.537595  \n",
       "KNeighborsClassifier          0.706496        2.053243        0.537595  \n",
       "LogisticRegression            0.706496        0.446879        0.537595  \n",
       "MLPClassifier                 0.706496        0.447017        0.537595  \n",
       "RandomForestClassifier        0.706496        1.923574        0.537595  "
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.7 ms\n"
     ]
    }
   ],
   "source": [
    "model_table.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 65.2 ms\n"
     ]
    }
   ],
   "source": [
    "new_data = remove_data(data, .95)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()\n",
    "first_word_encoded = encoder.fit_transform(new_data['first_word'])\n",
    "both = np.hstack((first_word_encoded, np.array(new_data['matches'])[:, np.newaxis]))\n",
    "thrice = np.hstack((both, np.array(new_data['last_word'])[:, np.newaxis]))\n",
    "cuatro = np.hstack((thrice, np.array(new_data['length'])[:, np.newaxis]))\n",
    "cinco = np.hstack((cuatro, np.array(new_data['number'])[:, np.newaxis]))\n",
    "seis = np.hstack((cinco, np.array(new_data['place'])[:, np.newaxis]))\n",
    "siete = np.hstack((seis, np.array(new_data['what_country'])[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 97.7 ms\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(seis, new_data['response'], random_state=state)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(xtrain, ytrain)\n",
    "ypred_test = clf.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7638983050847458, Naive Accuracy: 0.7430508474576272\n",
      "time: 87.1 ms\n"
     ]
    }
   ],
   "source": [
    "accuracies = 0\n",
    "naive = 0\n",
    "iters = 5\n",
    "for state in range(iters):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(seis, new_data['response'], random_state=state)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    ypred_test = clf.predict(xtest)\n",
    "    accuracies += accuracy_score(ytest, ypred_test)\n",
    "    naive += 1-np.mean(ytest)\n",
    "print(\"Model Accuracy: {}, Naive Accuracy: {}\".format(accuracies/iters, naive/iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.5051833492733436, Naive Accuracy: 0.5674004312670509\n",
      "time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "model_total = 0\n",
    "naive_total = 0\n",
    "iters = 10\n",
    "for state in range(iters):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(seis, new_data['response'], random_state=state)\n",
    "    ave_yes = np.mean(ytest)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    ypred_test = clf.predict_proba(xtest)\n",
    "    single_naive = np.array([[1-ave_yes, ave_yes], [1-ave_yes, ave_yes]])\n",
    "    naive_predictions = np.repeat(single_naive, [0, len(ytest)], axis=0)\n",
    "    model_total += log_loss(ytest, ypred_test)\n",
    "    naive_total += log_loss(ytest, naive_predictions)\n",
    "print(\"Model Accuracy: {}, Naive Accuracy: {}\".format(model_total/iters, naive_total/iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'how': 949,\n",
       "         'what': 2400,\n",
       "         'whatever': 2,\n",
       "         'when': 428,\n",
       "         'where': 362,\n",
       "         'who': 579})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.57 ms\n"
     ]
    }
   ],
   "source": [
    "Counter(data['first_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('what', 'is'), 4549),\n",
       " (('is', 'the'), 2167),\n",
       " (('how', 'many'), 1848),\n",
       " (('in', 'the'), 1226),\n",
       " (('what', 'are'), 1107),\n",
       " (('was', 'the'), 985),\n",
       " (('is', 'a'), 793),\n",
       " (('when', 'was'), 773),\n",
       " (('of', 'the'), 772),\n",
       " (('when', 'did'), 765),\n",
       " (('are', 'the'), 736),\n",
       " (('where', 'is'), 718),\n",
       " (('what', 'does'), 712),\n",
       " (('who', 'is'), 672),\n",
       " (('did', 'the'), 627),\n",
       " (('how', 'did'), 512),\n",
       " (('what', 'was'), 477),\n",
       " (('how', 'does'), 421),\n",
       " (('how', 'much'), 416),\n",
       " (('who', 'was'), 412),\n",
       " (('does', 'the'), 388),\n",
       " (('the', 'first'), 373),\n",
       " (('on', 'the'), 315),\n",
       " (('how', 'is'), 307),\n",
       " (('does', 'a'), 285),\n",
       " (('where', 'did'), 274),\n",
       " (('what', 'did'), 261),\n",
       " (('are', 'in'), 259),\n",
       " (('what', 'year'), 255),\n",
       " (('where', 'was'), 245),\n",
       " (('the', 'world'), 232),\n",
       " (('civil', 'war'), 231),\n",
       " (('in', 'a'), 223),\n",
       " (('how', 'old'), 218),\n",
       " (('what', 'country'), 211),\n",
       " (('used', 'for'), 204),\n",
       " (('how', 'do'), 201),\n",
       " (('the', 'us'), 197),\n",
       " (('do', 'you'), 194),\n",
       " (('what', 'do'), 194),\n",
       " (('the', 'civil'), 192),\n",
       " (('the', 'name'), 191),\n",
       " (('what', 'happened'), 190),\n",
       " (('how', 'long'), 189),\n",
       " (('to', 'the'), 188),\n",
       " (('is', 'an'), 188),\n",
       " (('what', 'kind'), 182),\n",
       " (('kind', 'of'), 178),\n",
       " (('the', 'most'), 178),\n",
       " (('where', 'are'), 177)]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 64.8 ms\n"
     ]
    }
   ],
   "source": [
    "Counter(bigram_list).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'name', 'is'),\n",
       " ('name', 'is', 'Jason'),\n",
       " ('is', 'Jason', 'Katz'),\n",
       " ('Jason', 'Katz', 'and'),\n",
       " ('Katz', 'and', 'I'),\n",
       " ('and', 'I', 'like'),\n",
       " ('I', 'like', 'football')]"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.35 ms\n"
     ]
    }
   ],
   "source": [
    "list(ngrams(word_tokenize('My name is Jason Katz and I like football'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "all_places = ''\n",
    "for state in us.states.STATES_AND_TERRITORIES:\n",
    "    all_places = all_places + ' ' + state.name\n",
    "for country in pycountry.countries:\n",
    "    all_places = all_places + ' ' + country.name\n",
    "all_places = re.sub('[(),]', '', all_places)\n",
    "places_words = word_tokenize(all_places.lower())\n",
    "places = set(places_words)\n",
    "for special in [\"'s\", 'the', 'part', 'of', 'and']:\n",
    "    places.remove(special)\n",
    "    \n",
    "countries = ''\n",
    "for country in pycountry.countries:\n",
    "    countries = countries + ' ' + country.name\n",
    "countries = re.sub('[(),]', '', countries)\n",
    "countries_words = word_tokenize(countries.lower())\n",
    "countries = set(countries_words)\n",
    "for special in [\"'s\", 'the', 'part', 'of', 'and']:\n",
    "    countries.remove(special)\n",
    "\n",
    "# Iterate through all question-sentence pairs\n",
    "data = {'what_country': [], 'country': [], 'first_word': [], 'length': [], 'matches': [], 'number': [], \n",
    "        'last_word': [], 'place': [], 'what_year': [], 'what_is': [], 'is': [], 'how_many': [], 'what_are': [], \n",
    "        'are': [], 'in_the': [], 'in': [], 'response': []}\n",
    "for index, row in train_data[0:].iterrows():\n",
    "    question = question_data.iloc[row['qIndex']]['question']\n",
    "    sentence = sentence_data.iloc[row['sIndex']]['sentence']\n",
    "\n",
    "    # Break the text into individual words\n",
    "    tokenized_question = [word.lower() for word in word_tokenize(question)]\n",
    "    tokenized_sentence = [word.lower() for word in word_tokenize(sentence)]\n",
    "\n",
    "    # Get first word of the question and number of words in the sentence\n",
    "    first_word_question = tokenized_question[0]\n",
    "    length_sentence = len(tokenized_sentence)\n",
    "    \n",
    "    \n",
    "    question_bigrams = list(bigrams(tokenized_question))\n",
    "    sentence_bigrams = list(bigrams(tokenized_sentence))\n",
    "    what_country = ('what', 'country') in question_bigrams\n",
    "    what_year = ('what', 'year') in question_bigrams\n",
    "    what_is = ('what', 'is') in question_bigrams\n",
    "    how_many = ('how', 'many') in question_bigrams\n",
    "    what_are = ('what', 'are') in question_bigrams\n",
    "    in_the = ('in', 'the') in question_bigrams\n",
    "    contains_is = \"is\" in tokenized_sentence\n",
    "    contains_in = \"in\" in tokenized_sentence\n",
    "    contains_are = \"are\" in tokenized_sentence\n",
    "\n",
    "    # Words to ignore (prepositions, pronouns, etc.)\n",
    "    common_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Count how many words from the question appear in the sentence\n",
    "    match = 0\n",
    "    for word in tokenized_question:\n",
    "        match += word in tokenized_sentence and word not in common_words\n",
    "\n",
    "    # Check if the sentence contains a number\n",
    "    contains_number = 0\n",
    "    for word in tokenized_sentence:\n",
    "        if word.isdigit():\n",
    "            contains_number = 1\n",
    "            break\n",
    "\n",
    "    # Check if the last word in the question appears in the sentence\n",
    "    last_word = tokenized_question[-1]\n",
    "    if last_word == '?':\n",
    "        last_word = tokenized_question[-2]\n",
    "\n",
    "    # Check if the sentence contains a place (country name or US state name)\n",
    "    contains_place = 0\n",
    "    for word in tokenized_sentence:\n",
    "        if word in places:\n",
    "            contains_place = 1\n",
    "            break\n",
    "            \n",
    "    # Check if the sentence contains a country\n",
    "    contains_country = 0\n",
    "    for word in tokenized_sentence:\n",
    "        if word in countries:\n",
    "            contains_country = 1\n",
    "            break\n",
    "\n",
    "    # Append metrics from question-sentence pair\n",
    "    data['response'].append(int(train_targets.iloc[index]['isAnswer']))\n",
    "    data['what_country'].append(int(what_country))\n",
    "    data['what_year'].append(int(what_year))\n",
    "    data['first_word'].append(first_word_question)\n",
    "    data['length'].append(length_sentence)\n",
    "    data['matches'].append(match)\n",
    "    data['number'].append(contains_number)\n",
    "    data['last_word'].append(int(last_word in tokenized_sentence))\n",
    "    data['place'].append(contains_place)\n",
    "    data['country'].append(contains_country)\n",
    "    data['what_is'].append(what_is)\n",
    "    data['is'].append(int(contains_is))\n",
    "    data['how_many'].append(int(how_many))\n",
    "    data['what_are'].append(int(what_are))\n",
    "    data['are'].append(int(contains_are))\n",
    "    data['in_the'].append(int(in_the))\n",
    "    data['in'].append(int(contains_in))\n",
    "\n",
    "# Write data to file\n",
    "with open('/Users/JasonKatz/Desktop/darpa_problems/r_32/data/data.json', 'w') as fd:\n",
    "    fd.write(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('how', 'are'): 131,\n",
       "         ('are', 'glacier'): 5,\n",
       "         ('glacier', 'caves'): 5,\n",
       "         ('caves', 'formed'): 5,\n",
       "         ('formed', '?'): 5,\n",
       "         ('are', 'the'): 736,\n",
       "         ('the', 'directions'): 7,\n",
       "         ('directions', 'of'): 7,\n",
       "         ('of', 'the'): 772,\n",
       "         ('the', 'velocity'): 7,\n",
       "         ('velocity', 'and'): 7,\n",
       "         ('and', 'force'): 7,\n",
       "         ('force', 'vectors'): 7,\n",
       "         ('vectors', 'related'): 7,\n",
       "         ('related', 'in'): 7,\n",
       "         ('in', 'a'): 223,\n",
       "         ('a', 'circular'): 7,\n",
       "         ('circular', 'motion'): 7,\n",
       "         ('how', 'did'): 512,\n",
       "         ('did', 'apollo'): 8,\n",
       "         ('apollo', 'creed'): 8,\n",
       "         ('creed', 'die'): 8,\n",
       "         ('how', 'long'): 189,\n",
       "         ('long', 'is'): 50,\n",
       "         ('is', 'the'): 2167,\n",
       "         ('the', 'term'): 42,\n",
       "         ('term', 'for'): 7,\n",
       "         ('for', 'federal'): 7,\n",
       "         ('federal', 'judges'): 7,\n",
       "         ('how', 'a'): 46,\n",
       "         ('a', 'beretta'): 3,\n",
       "         ('beretta', 'model'): 3,\n",
       "         ('model', '21'): 3,\n",
       "         ('21', 'pistols'): 3,\n",
       "         ('pistols', 'magazines'): 3,\n",
       "         ('magazines', 'works'): 3,\n",
       "         ('a', 'vul'): 14,\n",
       "         ('vul', 'works'): 14,\n",
       "         ('how', 'an'): 8,\n",
       "         ('an', 'outdoor'): 1,\n",
       "         ('outdoor', 'wood'): 1,\n",
       "         ('wood', 'boiler'): 1,\n",
       "         ('boiler', 'works'): 1,\n",
       "         ('how', 'big'): 94,\n",
       "         ('big', 'did'): 7,\n",
       "         ('did', 'girl'): 7,\n",
       "         ('girl', 'scout'): 7,\n",
       "         ('scout', 'cookie'): 7,\n",
       "         ('cookie', 'boxes'): 7,\n",
       "         ('boxes', 'used'): 7,\n",
       "         ('used', 'to'): 25,\n",
       "         ('to', 'be'): 90,\n",
       "         ('big', 'is'): 24,\n",
       "         ('the', 'purdue'): 11,\n",
       "         ('purdue', 'greek'): 11,\n",
       "         ('greek', 'system'): 11,\n",
       "         ('big', 'do'): 6,\n",
       "         ('do', 'sebaceous'): 6,\n",
       "         ('sebaceous', 'cysts'): 6,\n",
       "         ('cysts', 'get'): 6,\n",
       "         ('are', 'pointe'): 6,\n",
       "         ('pointe', 'shoes'): 6,\n",
       "         ('shoes', 'made'): 6,\n",
       "         ('how', 'much'): 416,\n",
       "         ('much', 'is'): 80,\n",
       "         ('is', '1'): 15,\n",
       "         ('1', 'tablespoon'): 10,\n",
       "         ('tablespoon', 'of'): 10,\n",
       "         ('of', 'water'): 10,\n",
       "         ('much', 'are'): 15,\n",
       "         ('the', 'harry'): 15,\n",
       "         ('harry', 'potter'): 61,\n",
       "         ('potter', 'movies'): 15,\n",
       "         ('movies', 'worth'): 15,\n",
       "         ('a', 'rocket'): 8,\n",
       "         ('rocket', 'engine'): 8,\n",
       "         ('engine', 'works'): 8,\n",
       "         ('how', 'old'): 218,\n",
       "         ('old', 'was'): 43,\n",
       "         ('was', 'britney'): 18,\n",
       "         ('britney', 'spears'): 18,\n",
       "         ('spears', 'when'): 18,\n",
       "         ('when', 'she'): 27,\n",
       "         ('she', 'recorded'): 18,\n",
       "         ('recorded', 'hit'): 18,\n",
       "         ('hit', 'me'): 18,\n",
       "         ('me', 'baby'): 18,\n",
       "         ('baby', 'one'): 18,\n",
       "         ('one', 'more'): 18,\n",
       "         ('more', 'time'): 25,\n",
       "         ('are', 'cholera'): 9,\n",
       "         ('cholera', 'and'): 9,\n",
       "         ('and', 'typhus'): 9,\n",
       "         ('typhus', 'transmitted'): 9,\n",
       "         ('transmitted', 'and'): 9,\n",
       "         ('and', 'prevented'): 9,\n",
       "         ('old', 'is'): 71,\n",
       "         ('is', 'sybil'): 13,\n",
       "         ('sybil', 'vane'): 13,\n",
       "         ('vane', 'in'): 13,\n",
       "         ('in', 'the'): 1226,\n",
       "         ('the', 'picture'): 13,\n",
       "         ('picture', 'of'): 13,\n",
       "         ('of', 'dorian'): 13,\n",
       "         ('dorian', 'gray'): 13,\n",
       "         ('is', 'zsa'): 10,\n",
       "         ('zsa', 'zsa'): 10,\n",
       "         ('zsa', 'gabor'): 10,\n",
       "         ('gabor', \"'s\"): 10,\n",
       "         (\"'s\", 'daughter'): 22,\n",
       "         ('how', 'bruce'): 14,\n",
       "         ('bruce', 'lee'): 14,\n",
       "         ('lee', 'died'): 14,\n",
       "         ('how', 'active'): 11,\n",
       "         ('active', 'can'): 11,\n",
       "         ('can', 'one'): 11,\n",
       "         ('one', 'be'): 11,\n",
       "         ('be', 'with'): 11,\n",
       "         ('with', 'copd'): 11,\n",
       "         ('did', 'anne'): 15,\n",
       "         ('anne', 'frank'): 15,\n",
       "         ('frank', 'die'): 15,\n",
       "         ('is', 'kurt'): 12,\n",
       "         ('kurt', 'cobain'): 24,\n",
       "         ('cobain', \"'s\"): 12,\n",
       "         ('are', 'aircraft'): 7,\n",
       "         ('aircraft', 'radial'): 7,\n",
       "         ('radial', 'engines'): 7,\n",
       "         ('engines', 'built'): 7,\n",
       "         ('old', 'were'): 31,\n",
       "         ('were', 'golden'): 7,\n",
       "         ('golden', 'girls'): 7,\n",
       "         ('girls', 'at'): 7,\n",
       "         ('at', 'time'): 7,\n",
       "         ('time', 'of'): 7,\n",
       "         ('of', 'show'): 7,\n",
       "         ('how', 'deep'): 42,\n",
       "         ('deep', 'can'): 28,\n",
       "         ('can', 'be'): 38,\n",
       "         ('be', 'drill'): 28,\n",
       "         ('drill', 'for'): 28,\n",
       "         ('for', 'deep'): 28,\n",
       "         ('deep', 'underwater'): 28,\n",
       "         ('was', 'monica'): 2,\n",
       "         ('monica', 'lewinsky'): 4,\n",
       "         ('lewinsky', 'during'): 2,\n",
       "         ('during', 'the'): 91,\n",
       "         ('the', 'affair'): 2,\n",
       "         ('long', 'was'): 65,\n",
       "         ('was', 'frank'): 16,\n",
       "         ('frank', 'sinatra'): 16,\n",
       "         ('sinatra', 'famous'): 16,\n",
       "         ('is', 'cornhole'): 4,\n",
       "         ('cornhole', 'hole'): 4,\n",
       "         ('is', 'a'): 793,\n",
       "         ('a', 'gold'): 9,\n",
       "         ('gold', '1986'): 2,\n",
       "         ('1986', 'american'): 2,\n",
       "         ('american', 'eagle'): 2,\n",
       "         ('eagle', 'worth'): 2,\n",
       "         ('worth', '?'): 2,\n",
       "         ('is', 'hot'): 3,\n",
       "         ('hot', 'yoga'): 3,\n",
       "         ('yoga', 'growing'): 3,\n",
       "         ('growing', 'yearly'): 3,\n",
       "         ('old', 'are'): 3,\n",
       "         ('the', 'massey'): 3,\n",
       "         ('massey', 'brothers'): 3,\n",
       "         ('is', '7'): 2,\n",
       "         ('7', 'teaspoons'): 2,\n",
       "         ('teaspoons', \"'\"): 2,\n",
       "         ('the', 'archegonia'): 10,\n",
       "         ('archegonia', 'of'): 10,\n",
       "         ('the', 'fern'): 10,\n",
       "         ('fern', 'and'): 10,\n",
       "         ('and', 'pine'): 10,\n",
       "         ('pine', 'cone'): 10,\n",
       "         ('cone', 'similar'): 10,\n",
       "         ('a', 'flat'): 30,\n",
       "         ('flat', 'membrane'): 30,\n",
       "         ('membrane', 'roof'): 30,\n",
       "         ('roof', 'good'): 30,\n",
       "         ('good', 'for'): 47,\n",
       "         ('for', '?'): 78,\n",
       "         ('how', 'cds'): 8,\n",
       "         ('cds', 'are'): 8,\n",
       "         ('are', 'read'): 8,\n",
       "         ('is', 'jk'): 15,\n",
       "         ('jk', 'rowling'): 15,\n",
       "         ('rowling', 'worth'): 15,\n",
       "         ('is', 'auburndale'): 4,\n",
       "         ('auburndale', 'florida'): 4,\n",
       "         ('are', 'tropical'): 30,\n",
       "         ('tropical', 'storms'): 23,\n",
       "         ('storms', 'and'): 23,\n",
       "         ('and', 'hurricanes'): 23,\n",
       "         ('hurricanes', 'named'): 23,\n",
       "         ('is', 'kirk'): 5,\n",
       "         ('kirk', 'douglas'): 5,\n",
       "         ('douglas', ','): 5,\n",
       "         (',', 'the'): 5,\n",
       "         ('the', 'actor'): 39,\n",
       "         ('actor', '?'): 5,\n",
       "         ('the', 'singer'): 32,\n",
       "         ('singer', 'bob'): 11,\n",
       "         ('bob', 'seger'): 11,\n",
       "         ('was', 'richard'): 23,\n",
       "         ('richard', 'nixon'): 28,\n",
       "         ('nixon', 'a'): 23,\n",
       "         ('a', 'president'): 23,\n",
       "         ('did', 'bleeding'): 21,\n",
       "         ('bleeding', 'sumner'): 21,\n",
       "         ('sumner', 'lead'): 21,\n",
       "         ('lead', 'to'): 30,\n",
       "         ('to', 'the'): 188,\n",
       "         ('the', 'civil'): 192,\n",
       "         ('civil', 'war'): 231,\n",
       "         ('are', 'business'): 12,\n",
       "         ('business', 'ethics'): 12,\n",
       "         ('ethics', 'different'): 12,\n",
       "         ('different', 'in'): 12,\n",
       "         ('in', 'africa'): 18,\n",
       "         ('africa', 'and'): 12,\n",
       "         ('and', 'united'): 12,\n",
       "         ('united', 'states'): 136,\n",
       "         ('how', 'bad'): 14,\n",
       "         ('bad', 'burns'): 14,\n",
       "         ('burns', 'go'): 14,\n",
       "         ('go', 'to'): 108,\n",
       "         ('to', 'burn'): 14,\n",
       "         ('burn', 'center'): 14,\n",
       "         ('deep', 'is'): 14,\n",
       "         ('the', 'atlantic'): 14,\n",
       "         ('atlantic', 'ocean'): 14,\n",
       "         ('are', 'storm'): 2,\n",
       "         ('storm', 'names'): 2,\n",
       "         ('names', 'determined'): 2,\n",
       "         ('determined', '?'): 9,\n",
       "         ('big', 'or'): 20,\n",
       "         ('or', 'small'): 20,\n",
       "         ('small', 'a'): 20,\n",
       "         ('a', 'visible'): 20,\n",
       "         ('visible', 'light'): 20,\n",
       "         ('light', 'can'): 20,\n",
       "         ('can', 'get'): 20,\n",
       "         ('big', 'are'): 13,\n",
       "         ('are', 'baby'): 13,\n",
       "         ('baby', 'zebras'): 13,\n",
       "         ('zebras', 'when'): 13,\n",
       "         ('when', 'born'): 13,\n",
       "         ('did', 'barry'): 6,\n",
       "         ('barry', 'white'): 6,\n",
       "         ('white', 'die'): 6,\n",
       "         ('die', '?'): 78,\n",
       "         ('did', 'angelina'): 12,\n",
       "         ('angelina', 'jolie'): 12,\n",
       "         ('jolie', 'get'): 12,\n",
       "         ('get', 'her'): 12,\n",
       "         ('her', 'name'): 12,\n",
       "         ('name', '?'): 29,\n",
       "         ('how', 'can'): 24,\n",
       "         ('can', 'plant'): 7,\n",
       "         ('plant', 'gases'): 7,\n",
       "         ('gases', 'slow'): 7,\n",
       "         ('slow', 'erosion'): 7,\n",
       "         ('was', 'angelina'): 17,\n",
       "         ('angelina', 'on'): 17,\n",
       "         ('on', 'the'): 315,\n",
       "         ('the', 'jersey'): 17,\n",
       "         ('jersey', 'shore'): 17,\n",
       "         ('shore', '?'): 17,\n",
       "         ('can', 'a'): 24,\n",
       "         ('a', 'territory'): 7,\n",
       "         ('territory', 'become'): 7,\n",
       "         ('become', 'a'): 35,\n",
       "         ('a', 'state'): 52,\n",
       "         ('was', 'arnold'): 5,\n",
       "         ('arnold', 'in'): 5,\n",
       "         ('in', 'terminator'): 5,\n",
       "         ('terminator', '3'): 5,\n",
       "         ('a', 'cord'): 5,\n",
       "         ('cord', 'of'): 5,\n",
       "         ('of', 'wood'): 9,\n",
       "         ('much', 'was'): 10,\n",
       "         ('was', 'game'): 10,\n",
       "         ('game', 'boy'): 10,\n",
       "         ('boy', 'color'): 10,\n",
       "         ('color', 'when'): 10,\n",
       "         ('when', 'it'): 10,\n",
       "         ('it', 'came'): 10,\n",
       "         ('came', 'out'): 46,\n",
       "         ('were', 'the'): 124,\n",
       "         ('the', 'twin'): 24,\n",
       "         ('twin', 'towers'): 24,\n",
       "         ('towers', 'when'): 24,\n",
       "         ('when', 'destroyed'): 24,\n",
       "         ('are', 'transverse'): 5,\n",
       "         ('transverse', 'ranges'): 5,\n",
       "         ('ranges', 'formed'): 5,\n",
       "         ('the', '#'): 11,\n",
       "         ('#', 'of'): 11,\n",
       "         ('of', 'electrons'): 11,\n",
       "         ('electrons', 'in'): 11,\n",
       "         ('in', 'each'): 24,\n",
       "         ('each', 'shell'): 11,\n",
       "         ('shell', 'determined'): 11,\n",
       "         ('was', 'anna'): 9,\n",
       "         ('anna', 'nicole'): 9,\n",
       "         ('nicole', 'smith'): 9,\n",
       "         ('smith', 'when'): 9,\n",
       "         ('she', 'meet'): 9,\n",
       "         ('meet', 'her'): 9,\n",
       "         ('her', 'old'): 9,\n",
       "         ('old', 'husband'): 9,\n",
       "         ('a', 'yen'): 7,\n",
       "         ('yen', 'coin'): 7,\n",
       "         ('how', 'tall'): 17,\n",
       "         ('tall', 'are'): 17,\n",
       "         ('the', 'members'): 17,\n",
       "         ('members', 'of'): 17,\n",
       "         ('of', 'tlc'): 17,\n",
       "         ('the', 'tarzan'): 7,\n",
       "         ('tarzan', 'movie'): 7,\n",
       "         ('an', 'oil'): 7,\n",
       "         ('oil', 'rig'): 7,\n",
       "         ('rig', 'works'): 7,\n",
       "         ('are', 'sheep'): 2,\n",
       "         ('sheep', 'slaughtered'): 2,\n",
       "         ('how', 'many'): 1848,\n",
       "         ('many', 'presidents'): 27,\n",
       "         ('presidents', 'of'): 20,\n",
       "         ('the', 'us'): 197,\n",
       "         ('a', 'silencer'): 21,\n",
       "         ('silencer', 'works'): 21,\n",
       "         ('is', 'beatrice'): 3,\n",
       "         ('beatrice', 'author'): 3,\n",
       "         ('is', 'tekken'): 6,\n",
       "         ('tekken', 'blood'): 6,\n",
       "         ('blood', 'vengeance'): 6,\n",
       "         ('vengeance', 'movie'): 6,\n",
       "         ('can', 'hoa'): 10,\n",
       "         ('hoa', 'collect'): 10,\n",
       "         ('collect', 'unpaid'): 10,\n",
       "         ('unpaid', 'fees'): 10,\n",
       "         ('fees', 'on'): 10,\n",
       "         ('on', 'property'): 10,\n",
       "         ('big', 'can'): 24,\n",
       "         ('can', 'texel'): 24,\n",
       "         ('texel', 'guinea'): 24,\n",
       "         ('guinea', 'pigs'): 24,\n",
       "         ('pigs', 'become'): 24,\n",
       "         ('are', 'public'): 2,\n",
       "         ('public', 'schools'): 2,\n",
       "         ('schools', 'funded'): 2,\n",
       "         ('are', 'ribosomes'): 21,\n",
       "         ('ribosomes', 'made'): 21,\n",
       "         ('what', 'happens'): 72,\n",
       "         ('happens', 'to'): 35,\n",
       "         ('the', 'light'): 10,\n",
       "         ('light', 'independent'): 10,\n",
       "         ('independent', 'reactions'): 10,\n",
       "         ('reactions', 'of'): 20,\n",
       "         ('of', 'photosynthesis'): 20,\n",
       "         ('photosynthesis', '?'): 20,\n",
       "         ('how', 'do'): 201,\n",
       "         ('do', 'you'): 194,\n",
       "         ('you', 'find'): 28,\n",
       "         ('find', 'the'): 12,\n",
       "         ('the', 'mean'): 12,\n",
       "         ('mean', 'of'): 12,\n",
       "         ('the', 'squares'): 12,\n",
       "         ('squares', 'of'): 12,\n",
       "         ('the', 'first'): 373,\n",
       "         ('first', '10'): 12,\n",
       "         ('10', 'counting'): 12,\n",
       "         ('counting', 'numbers'): 12,\n",
       "         ('what', 'ended'): 10,\n",
       "         ('ended', 'the'): 10,\n",
       "         ('the', 'era'): 10,\n",
       "         ('era', 'of'): 10,\n",
       "         ('of', 'good'): 16,\n",
       "         ('good', 'feelings'): 10,\n",
       "         ('what', 'did'): 261,\n",
       "         ('did', 'mia'): 8,\n",
       "         ('mia', 'hamm'): 8,\n",
       "         ('hamm', 'do'): 8,\n",
       "         ('do', 'his'): 8,\n",
       "         ('his', 'work'): 8,\n",
       "         ('did', 'the'): 627,\n",
       "         ('the', 'new'): 72,\n",
       "         ('new', 'york'): 61,\n",
       "         ('york', 'red'): 13,\n",
       "         ('red', 'bulls'): 13,\n",
       "         ('bulls', 'started'): 13,\n",
       "         ('started', 'in'): 21,\n",
       "         ('the', 'mls'): 21,\n",
       "         ('what', 'countries'): 109,\n",
       "         ('countries', 'are'): 79,\n",
       "         ('are', 'under'): 23,\n",
       "         ('under', 'the'): 23,\n",
       "         ('the', 'buddhism'): 23,\n",
       "         ('buddhism', 'religion'): 23,\n",
       "         ('did', 'wild'): 6,\n",
       "         ('wild', 'bill'): 6,\n",
       "         ('bill', \"'s\"): 6,\n",
       "         (\"'s\", 'father'): 31,\n",
       "         ('father', 'die'): 6,\n",
       "         ('many', 'land'): 7,\n",
       "         ('land', 'rovers'): 7,\n",
       "         ('rovers', 'have'): 7,\n",
       "         ('have', 'landed'): 7,\n",
       "         ('landed', 'on'): 7,\n",
       "         ('on', 'mars'): 7,\n",
       "         ('do', 'forensic'): 10,\n",
       "         ('forensic', 'auditors'): 10,\n",
       "         ('auditors', 'examine'): 10,\n",
       "         ('examine', 'financial'): 10,\n",
       "         ('financial', 'reporting'): 10,\n",
       "         ('what', 'does'): 712,\n",
       "         ('does', 'a'): 285,\n",
       "         ('a', 'silver'): 7,\n",
       "         ('silver', 'dollar'): 7,\n",
       "         ('dollar', '1873'): 7,\n",
       "         ('1873', 'coin'): 7,\n",
       "         ('coin', 'look'): 7,\n",
       "         ('look', 'like'): 25,\n",
       "         ('how', 'fast'): 27,\n",
       "         ('fast', 'is'): 24,\n",
       "         ('is', 'one'): 24,\n",
       "         ('one', 'g'): 24,\n",
       "         ('the', 'augurs'): 6,\n",
       "         ('augurs', 'use'): 6,\n",
       "         ('use', 'to'): 16,\n",
       "         ('to', 'interpret'): 6,\n",
       "         ('interpret', 'the'): 6,\n",
       "         ('the', 'will'): 6,\n",
       "         ('will', 'of'): 6,\n",
       "         ('the', 'gods'): 6,\n",
       "         ('gods', '?'): 6,\n",
       "         ('what', 'county'): 125,\n",
       "         ('county', 'is'): 124,\n",
       "         ('is', 'farmington'): 11,\n",
       "         ('farmington', 'hills'): 11,\n",
       "         ('hills', ','): 11,\n",
       "         (',', 'mi'): 11,\n",
       "         ('mi', 'in'): 11,\n",
       "         ('in', '?'): 115,\n",
       "         ('how', 'were'): 27,\n",
       "         ('were', 'letters'): 12,\n",
       "         ('letters', 'sealed'): 12,\n",
       "         ('sealed', 'in'): 12,\n",
       "         ('the', '1800s'): 12,\n",
       "         ('a', 'groundhog'): 7,\n",
       "         ('groundhog', 'look'): 7,\n",
       "         ('look', 'for'): 11,\n",
       "         ('for', 'on'): 12,\n",
       "         ('on', 'groundhog'): 7,\n",
       "         ('groundhog', 'day'): 7,\n",
       "         ('how', 'is'): 307,\n",
       "         ('the', 'weather'): 10,\n",
       "         ('weather', 'in'): 10,\n",
       "         ('in', 'tampa'): 33,\n",
       "         ('what', 'generates'): 8,\n",
       "         ('generates', 'gamma'): 8,\n",
       "         ('gamma', 'radiation'): 8,\n",
       "         ('many', 'presidential'): 24,\n",
       "         ('presidential', 'terms'): 24,\n",
       "         ('terms', 'did'): 34,\n",
       "         ('did', 'fdr'): 24,\n",
       "         ('fdr', 'serve'): 24,\n",
       "         ('what', 'committees'): 2,\n",
       "         ('committees', 'are'): 2,\n",
       "         ('are', 'joint'): 2,\n",
       "         ('joint', 'committees'): 2,\n",
       "         ('many', 'brothels'): 9,\n",
       "         ('brothels', 'were'): 9,\n",
       "         ('were', 'there'): 24,\n",
       "         ('there', 'in'): 69,\n",
       "         ('the', 'united'): 96,\n",
       "         ('states', 'in'): 27,\n",
       "         ('in', '1840'): 9,\n",
       "         ('1840', \"'s\"): 9,\n",
       "         ('many', 'stripes'): 6,\n",
       "         ('stripes', 'on'): 3,\n",
       "         ('the', 'flag'): 19,\n",
       "         ('many', 'states'): 28,\n",
       "         ('states', 'and'): 2,\n",
       "         ('and', 'territories'): 2,\n",
       "         ('territories', 'are'): 2,\n",
       "         ('are', 'within'): 2,\n",
       "         ('within', 'india'): 2,\n",
       "         ('india', '?'): 2,\n",
       "         ('what', 'congressional'): 15,\n",
       "         ('congressional', 'district'): 15,\n",
       "         ('district', 'am'): 15,\n",
       "         ('am', 'i'): 18,\n",
       "         ('i', 'in'): 15,\n",
       "         ('many', 'people'): 176,\n",
       "         ('people', 'have'): 25,\n",
       "         ('have', 'mariah'): 20,\n",
       "         ('mariah', 'carey'): 40,\n",
       "         ('carey', 'dated'): 20,\n",
       "         ('dated', '?'): 20,\n",
       "         ('much', 'does'): 110,\n",
       "         ('does', 'cloud'): 17,\n",
       "         ('cloud', '10'): 17,\n",
       "         ('10', 'cost'): 17,\n",
       "         ('cost', 'wikipedia'): 17,\n",
       "         ('is', 'galveston'): 5,\n",
       "         ('galveston', 'in'): 5,\n",
       "         ('in', 'texas'): 13,\n",
       "         ('what', 'cities'): 15,\n",
       "         ('cities', 'are'): 15,\n",
       "         ('are', 'in'): 259,\n",
       "         ('the', 'bahamas'): 4,\n",
       "         ('many', 'schools'): 7,\n",
       "         ('schools', 'are'): 31,\n",
       "         ('the', 'big'): 18,\n",
       "         ('big', 'ten'): 7,\n",
       "         ('is', 'single'): 11,\n",
       "         ('single', 'malt'): 11,\n",
       "         ('malt', 'scotch'): 11,\n",
       "         ('scotch', 'made'): 11,\n",
       "         ('what', 'composer'): 13,\n",
       "         ('composer', 'used'): 13,\n",
       "         ('used', 'sound'): 13,\n",
       "         ('sound', 'mass'): 13,\n",
       "         ('what', 'do'): 194,\n",
       "         ('you', 'call'): 5,\n",
       "         ('call', 'to'): 5,\n",
       "         ('the', 'grade'): 5,\n",
       "         ('grade', 'six'): 5,\n",
       "         ('six', 'pupils'): 5,\n",
       "         ('pupils', '?'): 5,\n",
       "         ('what', 'continent'): 10,\n",
       "         ('continent', 'is'): 10,\n",
       "         ('is', 'australia'): 10,\n",
       "         ('how', 'does'): 421,\n",
       "         ('does', 'rainfall'): 8,\n",
       "         ('rainfall', 'in'): 8,\n",
       "         ('in', 'new'): 42,\n",
       "         ('york', 'affect'): 8,\n",
       "         ('affect', 'the'): 45,\n",
       "         ('the', 'environment'): 8,\n",
       "         ('environment', '?'): 8,\n",
       "         ('people', 'live'): 27,\n",
       "         ('live', 'in'): 42,\n",
       "         ('in', 'atlanta'): 10,\n",
       "         ('atlanta', 'georgia'): 10,\n",
       "         ('the', 'hollywood'): 13,\n",
       "         ('hollywood', 'blacklist'): 13,\n",
       "         ('blacklist', 'have'): 13,\n",
       "         ('have', 'to'): 44,\n",
       "         ('to', 'do'): 23,\n",
       "         ('do', 'with'): 32,\n",
       "         ('with', 'mccarthy'): 13,\n",
       "         ('does', 'frederick'): 3,\n",
       "         ('frederick', 'trick'): 3,\n",
       "         ('trick', 'napoleon'): 3,\n",
       "         ('napoleon', 'in'): 3,\n",
       "         ('in', 'animal'): 3,\n",
       "         ('animal', 'farm'): 3,\n",
       "         ('many', 'days'): 8,\n",
       "         ('days', 'it'): 5,\n",
       "         ('it', 'take'): 14,\n",
       "         ('take', 'for'): 7,\n",
       "         ('for', 'neptune'): 5,\n",
       "         ('neptune', 'to'): 5,\n",
       "         ('to', 'orbit'): 5,\n",
       "         ('orbit', 'once'): 5,\n",
       "         ('many', '1969'): 2,\n",
       "         ('1969', 'dodge'): 2,\n",
       "         ('dodge', 'coronets'): 2,\n",
       "         ('coronets', 'were'): 2,\n",
       "         ('were', 'made'): 2,\n",
       "         ('made', '?'): 31,\n",
       "         ('many', 'mhz'): 1,\n",
       "         ('mhz', 'is'): 1,\n",
       "         ('is', 'arm9'): 1,\n",
       "         ('arm9', 'base'): 1,\n",
       "         ('does', 'it'): 52,\n",
       "         ('it', 'mean'): 22,\n",
       "         ('mean', 'to'): 33,\n",
       "         ('be', 'a'): 12,\n",
       "         ('a', 'commonwealth'): 4,\n",
       "         ('commonwealth', 'state'): 4,\n",
       "         ('people', 'died'): 17,\n",
       "         ('died', 'at'): 17,\n",
       "         ('at', 'the'): 48,\n",
       "         ('the', 'pentagon'): 17,\n",
       "         ('pentagon', 'in'): 17,\n",
       "         ('in', '9'): 17,\n",
       "         ('9', '11'): 17,\n",
       "         ('people', 'visit'): 13,\n",
       "         ('visit', 'crater'): 13,\n",
       "         ('crater', 'lake'): 13,\n",
       "         ('lake', 'national'): 13,\n",
       "         ('national', 'park'): 13,\n",
       "         ('park', 'each'): 13,\n",
       "         ('each', 'year'): 13,\n",
       "         ('long', 'can'): 4,\n",
       "         ('can', 'you'): 18,\n",
       "         ('you', 'be'): 11,\n",
       "         ('be', 'in'): 4,\n",
       "         ('the', 'supreme'): 41,\n",
       "         ('supreme', 'court'): 50,\n",
       "         ('county', 'in'): 3,\n",
       "         ('texas', 'is'): 8,\n",
       "         ('is', 'conroe'): 3,\n",
       "         ('conroe', 'located'): 3,\n",
       "         ('located', 'in'): 31,\n",
       "         ('what', 'edition'): 9,\n",
       "         ('edition', 'of'): 9,\n",
       "         ('of', 'book'): 9,\n",
       "         ('book', 'of'): 18,\n",
       "         ('of', 'mormon'): 9,\n",
       "         ('mormon', 'had'): 9,\n",
       "         ('had', '103'): 9,\n",
       "         ('103', 'sections'): 9,\n",
       "         ('how', 'high'): 3,\n",
       "         ('high', 'can'): 3,\n",
       "         ('can', 'enlisted'): 3,\n",
       "         ('enlisted', 'soldiers'): 3,\n",
       "         ('soldiers', 'get'): 3,\n",
       "         ('get', 'in'): 37,\n",
       "         ('the', 'marines'): 3,\n",
       "         ('what', 'a'): 13,\n",
       "         ('a', 'wonderful'): 18,\n",
       "         ('wonderful', 'world'): 10,\n",
       "         ('world', 'covers'): 5,\n",
       "         ('did', 'movie'): 16,\n",
       "         ('movie', 'theaters'): 23,\n",
       "         ('theaters', 'do'): 16,\n",
       "         ('do', 'for'): 32,\n",
       "         ('for', 'sound'): 16,\n",
       "         ('sound', 'before'): 16,\n",
       "         ('before', 'synchronized'): 16,\n",
       "         ('synchronized', 'sound'): 16,\n",
       "         ('sound', 'was'): 16,\n",
       "         ('was', 'introduced'): 37,\n",
       "         ('introduced', 'into'): 37,\n",
       "         ('into', 'film'): 16,\n",
       "         ('how', 'people'): 1,\n",
       "         ('what', 'group'): 4,\n",
       "         ('group', 'took'): 4,\n",
       "         ('took', 'home'): 4,\n",
       "         ('home', 'the'): 4,\n",
       "         ('the', 'award'): 4,\n",
       "         ('award', 'for'): 8,\n",
       "         ('for', 'best'): 8,\n",
       "         ('best', 'rock'): 4,\n",
       "         ('rock', 'album'): 4,\n",
       "         ('album', 'at'): 4,\n",
       "         ('the', 'australian'): 4,\n",
       "         ('australian', 'recording'): 4,\n",
       "         ('recording', 'industry'): 4,\n",
       "         ('industry', 'association'): 4,\n",
       "         ('association', '('): 4,\n",
       "         ('(', 'aria'): 4,\n",
       "         ('aria', ')'): 4,\n",
       "         (')', 'music'): 4,\n",
       "         ('music', 'awards'): 9,\n",
       "         ('awards', '?'): 4,\n",
       "         ('many', 'for'): 12,\n",
       "         ('for', 'profit'): 12,\n",
       "         ('profit', 'colleges'): 12,\n",
       "         ('colleges', 'in'): 12,\n",
       "         ('the', 'usa'): 58,\n",
       "         ('did', 'congress'): 4,\n",
       "         ('congress', 'vote'): 4,\n",
       "         ('vote', 'on'): 4,\n",
       "         ('the', '22nd'): 4,\n",
       "         ('22nd', 'amendment'): 4,\n",
       "         ('many', 'episodes'): 56,\n",
       "         ('episodes', 'of'): 50,\n",
       "         ('of', 'bo'): 5,\n",
       "         ('bo', 'bo'): 30,\n",
       "         ('what', 'building'): 12,\n",
       "         ('building', 'inside'): 9,\n",
       "         ('inside', 'central'): 9,\n",
       "         ('central', 'park'): 9,\n",
       "         ('what', 'carrier'): 3,\n",
       "         ('carrier', 'does'): 3,\n",
       "         ('does', 'straight'): 3,\n",
       "         ('straight', 'talk'): 3,\n",
       "         ('talk', 'use'): 3,\n",
       "         ('a', 'dredge'): 13,\n",
       "         ('dredge', 'work'): 13,\n",
       "         ('work', '?'): 23,\n",
       "         ('do', 'former'): 14,\n",
       "         ('former', 'volcanoes'): 14,\n",
       "         ('volcanoes', 'affect'): 14,\n",
       "         ('affect', 'hydrothermal'): 14,\n",
       "         ('hydrothermal', 'activity'): 14,\n",
       "         ('did', 'benedict'): 21,\n",
       "         ('benedict', 'arnold'): 21,\n",
       "         ('arnold', 'die'): 21,\n",
       "         ('many', 'world'): 6,\n",
       "         ('world', 'series'): 25,\n",
       "         ('series', 'did'): 6,\n",
       "         ('did', 'curt'): 6,\n",
       "         ('curt', 'schilling'): 6,\n",
       "         ('schilling', 'have'): 6,\n",
       "         ('many', 'star'): 17,\n",
       "         ('star', 'wars'): 22,\n",
       "         ('wars', 'movies'): 17,\n",
       "         ('movies', 'are'): 17,\n",
       "         ('are', 'there'): 102,\n",
       "         ('much', 'will'): 4,\n",
       "         ('will', 'a'): 11,\n",
       "         ('a', 'transfusion'): 4,\n",
       "         ('transfusion', 'of'): 4,\n",
       "         ('of', 'platelet'): 4,\n",
       "         ('platelet', 'apheresis'): 4,\n",
       "         ('apheresis', 'raise'): 4,\n",
       "         ('raise', 'the'): 4,\n",
       "         ('the', 'platelet'): 4,\n",
       "         ('platelet', 'count'): 4,\n",
       "         ('many', 'ports'): 11,\n",
       "         ('ports', 'are'): 11,\n",
       "         ('in', 'networking'): 11,\n",
       "         ('what', 'area'): 17,\n",
       "         ('area', 'code'): 23,\n",
       "         ('code', 'is'): 17,\n",
       "         ('is', '479'): 3,\n",
       "         ('does', 'arraignment'): 11,\n",
       "         ('arraignment', 'mean'): 11,\n",
       "         ('did', 'chaucer'): 4,\n",
       "         ('chaucer', 'do'): 4,\n",
       "         ('much', 'chicago'): 10,\n",
       "         ('chicago', 'snow'): 10,\n",
       "         ('snow', 'did'): 10,\n",
       "         ('did', 'we'): 10,\n",
       "         ('we', 'get'): 10,\n",
       "         ('in', '1979'): 10,\n",
       "         ('does', 'am'): 3,\n",
       "         ('am', 'and'): 5,\n",
       "         ('and', 'pm'): 3,\n",
       "         ('pm', 'stand'): 3,\n",
       "         ('stand', 'for'): 66,\n",
       "         ('is', 'canada'): 16,\n",
       "         ('canada', \"'s\"): 23,\n",
       "         (\"'s\", 'health'): 16,\n",
       "         ('health', 'care'): 49,\n",
       "         ('the', 'tarantula'): 3,\n",
       "         ('tarantula', 'get'): 3,\n",
       "         ('get', 'its'): 3,\n",
       "         ('its', 'name'): 3,\n",
       "         ('many', 'pounds'): 18,\n",
       "         ('pounds', 'is'): 18,\n",
       "         ('a', 'ton'): 18,\n",
       "         ('ton', '?'): 18,\n",
       "         ('many', 'series'): 22,\n",
       "         ('series', 'in'): 22,\n",
       "         ('in', 'hockey'): 36,\n",
       "         ('hockey', 'before'): 22,\n",
       "         ('before', 'the'): 32,\n",
       "         ('the', 'stanley'): 22,\n",
       "         ('stanley', 'cup'): 22,\n",
       "         ('do', 'pineapples'): 5,\n",
       "         ('pineapples', 'grow'): 5,\n",
       "         ('many', 'bytes'): 5,\n",
       "         ('bytes', 'in'): 5,\n",
       "         ('in', 'an'): 35,\n",
       "         ('an', 'unsigned'): 5,\n",
       "         ('unsigned', 'int'): 5,\n",
       "         ('int', 'c++'): 5,\n",
       "         ('it', 'cost'): 11,\n",
       "         ('cost', 'to'): 11,\n",
       "         ('to', 'set'): 25,\n",
       "         ('set', 'up'): 11,\n",
       "         ('up', 'hydroelectric'): 11,\n",
       "         ('what', 'country'): 211,\n",
       "         ('country', 'did'): 21,\n",
       "         ('did', 'wine'): 21,\n",
       "         ('wine', 'originate'): 21,\n",
       "         ('originate', 'in'): 21,\n",
       "         ('many', 'district'): 17,\n",
       "         ('district', 'are'): 17,\n",
       "         ('in', 'west'): 17,\n",
       "         ('west', 'bengal'): 17,\n",
       "         ('what', 'caribbean'): 10,\n",
       "         ('caribbean', 'island'): 10,\n",
       "         ('island', 'is'): 10,\n",
       "         ('is', 'part'): 10,\n",
       "         ('part', 'of'): 164,\n",
       "         ('of', 'france'): 30,\n",
       "         ('france', '?'): 14,\n",
       "         ('many', 'towns'): 6,\n",
       "         ('towns', 'in'): 6,\n",
       "         ('new', 'hampshire'): 6,\n",
       "         ('the', 'african'): 13,\n",
       "         ('african', 'slave'): 13,\n",
       "         ('slave', 'trade'): 26,\n",
       "         ('trade', 'affect'): 13,\n",
       "         ('affect', 'africans'): 13,\n",
       "         ('africans', 'in'): 13,\n",
       "         ('in', 'terms'): 13,\n",
       "         ('terms', 'of'): 13,\n",
       "         ('of', 'economy'): 13,\n",
       "         ('many', 'consecutive'): 29,\n",
       "         ('consecutive', 'starts'): 12,\n",
       "         ('starts', 'did'): 12,\n",
       "         ('did', 'brett'): 24,\n",
       "         ('brett', 'favre'): 24,\n",
       "         ('favre', 'have'): 12,\n",
       "         ('have', 'this'): 12,\n",
       "         ('this', 'season'): 12,\n",
       "         ('season', '?'): 12,\n",
       "         ('does', 'salesforce.com'): 5,\n",
       "         ('salesforce.com', 'do'): 5,\n",
       "         ('do', '?'): 23,\n",
       "         ('long', 'does'): 14,\n",
       "         ('for', 'seahorses'): 2,\n",
       "         ('seahorses', 'to'): 2,\n",
       "         ('to', 'mate'): 2,\n",
       "         ('mate', '?'): 2,\n",
       "         ('does', '1'): 21,\n",
       "         ('1', 'year'): 21,\n",
       "         ('year', 'mean'): 21,\n",
       "         ('the', 'planet'): 33,\n",
       "         ('what', 'cords'): 7,\n",
       "         ('cords', 'are'): 7,\n",
       "         ('are', 'used'): 25,\n",
       "         ('used', 'for'): 204,\n",
       "         ('for', 'in'): 7,\n",
       "         ('in', 'wiccan'): 7,\n",
       "         ('what', 'is'): 4549,\n",
       "         ('is', 'feedback'): 11,\n",
       "         ('feedback', 'mechanism'): 11,\n",
       "         ('mechanism', 'in'): 11,\n",
       "         ('in', 'plants'): 11,\n",
       "         ('plants', 'during'): 11,\n",
       "         ('during', 'respiration'): 11,\n",
       "         ('how', 'fire'): 22,\n",
       "         ('fire', 'extinguisher'): 22,\n",
       "         ('extinguisher', 'works'): 22,\n",
       "         ('did', 'native'): 30,\n",
       "         ('native', 'americans'): 38,\n",
       "         ('americans', 'do'): 30,\n",
       "         ('do', 'all'): 30,\n",
       "         ('all', 'day'): 30,\n",
       "         ('in', 'central'): 9,\n",
       "         ('central', 'america'): 9,\n",
       "         ('america', '?'): 36,\n",
       "         ('did', 'scrubs'): 17,\n",
       "         ('scrubs', 'end'): 17,\n",
       "         ('a', 'steam'): 23,\n",
       "         ('steam', 'engine'): 23,\n",
       "         ('engine', 'work'): 23,\n",
       "         ('what', 'key'): 13,\n",
       "         ('key', 'is'): 20,\n",
       "         ('is', 'ludacris'): 13,\n",
       "         ('ludacris', 'move'): 13,\n",
       "         ('move', 'bitch'): 13,\n",
       "         ('bitch', 'in'): 13,\n",
       "         ('to', 'queen'): 17,\n",
       "         ('queen', 'elizabeth'): 34,\n",
       "         ('elizabeth', \"'s\"): 17,\n",
       "         (\"'s\", 'old'): 17,\n",
       "         ('old', 'dresses'): 17,\n",
       "         ('is', 'christianity'): 13,\n",
       "         ('christianity', 'viewed'): 13,\n",
       "         ('viewed', 'in'): 13,\n",
       "         ('in', 'australia'): 13,\n",
       "         ('many', 'miles'): 24,\n",
       "         ('miles', 'is'): 17,\n",
       "         ('is', 'harvard'): 17,\n",
       "         ('harvard', 'university'): 17,\n",
       "         ('university', 'from'): 17,\n",
       "         ('from', 'yale'): 17,\n",
       "         ('yale', 'university'): 17,\n",
       "         ('university', '?'): 17,\n",
       "         ('does', 'sagebrush'): 8,\n",
       "         ('sagebrush', 'effect'): 8,\n",
       "         ('effect', 'community'): 8,\n",
       "         ('community', 'idaho'): 8,\n",
       "         ('many', 'users'): 15,\n",
       "         ('users', 'on'): 9,\n",
       "         ('on', 'mail.com'): 9,\n",
       "         ('is', 'tacoma'): 20,\n",
       "         ('tacoma', 'washington'): 20,\n",
       "         ('washington', 'in'): 20,\n",
       "         ('is', 'chula'): 7,\n",
       "         ('chula', 'vista'): 7,\n",
       "         ('vista', 'ca'): 7,\n",
       "         ('ca', 'in'): 10,\n",
       "         ('does', 'metes'): 11,\n",
       "         ('metes', 'and'): 11,\n",
       "         ('and', 'bounds'): 11,\n",
       "         ('bounds', 'mean'): 11,\n",
       "         ('did', 'isaac'): 14,\n",
       "         ('isaac', 'newton'): 14,\n",
       "         ('newton', 'do'): 14,\n",
       "         ('does', 'the'): 388,\n",
       "         ('the', 'universal'): 17,\n",
       "         ('universal', 'law'): 17,\n",
       "         ('law', 'of'): 36,\n",
       "         ('of', 'gravitation'): 17,\n",
       "         ('gravitation', 'state'): 17,\n",
       "         ('many', 'us'): 14,\n",
       "         ('us', 'soldiers'): 13,\n",
       "         ('soldiers', 'killed'): 13,\n",
       "         ('killed', 'in'): 50,\n",
       "         ('in', 'iraq'): 13,\n",
       "         ('iraq', 'war'): 13,\n",
       "         ('the', 'highwaymen'): 8,\n",
       "         ('highwaymen', 'get'): 8,\n",
       "         ('get', 'their'): 29,\n",
       "         ('their', 'name'): 8,\n",
       "         ('name', '1800'): 8,\n",
       "         ('did', 'magic'): 14,\n",
       "         ('magic', 'johnson'): 14,\n",
       "         ('johnson', 'get'): 14,\n",
       "         ('get', 'aids'): 14,\n",
       "         ('a', 'cell'): 28,\n",
       "         ('cell', 'function'): 13,\n",
       "         ('does', 'grecian'): 2,\n",
       "         ('grecian', 'formula'): 2,\n",
       "         ('formula', 'work'): 2,\n",
       "         ('what', 'are'): 1107,\n",
       "         ('are', 'projection'): 8,\n",
       "         ('projection', 'headlights'): 8,\n",
       "         ('did', 'women'): 8,\n",
       "         ('women', 'work'): 4,\n",
       "         ('work', 'for'): 16,\n",
       "         ('for', 'equal'): 4,\n",
       "         ('equal', 'rights'): 4,\n",
       "         ('rights', 'during'): 4,\n",
       "         ('the', 'women'): 29,\n",
       "         ('women', \"'s\"): 29,\n",
       "         (\"'s\", 'movement'): 4,\n",
       "         ('movement', '?'): 9,\n",
       "         ('?', 'were'): 4,\n",
       "         ('were', 'their'): 4,\n",
       "         ('their', 'efforts'): 4,\n",
       "         ('efforts', 'successful'): 4,\n",
       "         ('successful', '?'): 4,\n",
       "         ('many', 'times'): 59,\n",
       "         ('times', 'has'): 24,\n",
       "         ('has', 'whoopi'): 13,\n",
       "         ('whoopi', 'been'): 13,\n",
       "         ('been', 'married'): 13,\n",
       "         ('many', 'professional'): 10,\n",
       "         ('professional', 'hockey'): 10,\n",
       "         ('hockey', 'teams'): 10,\n",
       "         ('teams', 'in'): 10,\n",
       "         ('in', 'canada'): 29,\n",
       "         ('do', 'maggots'): 7,\n",
       "         ('maggots', 'turn'): 7,\n",
       "         ('turn', 'into'): 33,\n",
       "         ('into', 'to'): 7,\n",
       "         ('to', '?'): 27,\n",
       "         ('did', 'king'): 7,\n",
       "         ('king', 'herod'): 7,\n",
       "         ('herod', 'die'): 7,\n",
       "         ('what', 'antigen'): 4,\n",
       "         ('antigen', 'in'): 4,\n",
       "         ('in', 'present'): 4,\n",
       "         ('present', 'in'): 4,\n",
       "         ('in', 'type'): 4,\n",
       "         ('type', 'o+'): 4,\n",
       "         ('o+', 'and'): 4,\n",
       "         ('and', 'o-'): 4,\n",
       "         ('o-', 'blood'): 4,\n",
       "         ('what', 'happened'): 190,\n",
       "         ('happened', 'in'): 13,\n",
       "         ('in', '1907'): 1,\n",
       "         ('is', 'northville'): 7,\n",
       "         ('northville', 'mi'): 7,\n",
       "         ('does', 'strith'): 5,\n",
       "         ('episodes', 'are'): 7,\n",
       "         ('first', 'season'): 7,\n",
       "         ('season', 'of'): 10,\n",
       "         ('of', 'misfits'): 7,\n",
       "         ('many', 'games'): 12,\n",
       "         ('games', 'did'): 29,\n",
       "         ('favre', 'start'): 12,\n",
       "         ('start', 'in'): 36,\n",
       "         ('a', 'row'): 12,\n",
       "         ('us', 'planes'): 1,\n",
       "         ('planes', 'were'): 1,\n",
       "         ('were', 'hijacked'): 1,\n",
       "         ('hijacked', 'to'): 1,\n",
       "         ('to', 'cuba'): 1,\n",
       "         ('cuba', '?'): 1,\n",
       "         ('many', 'websites'): 4,\n",
       "         ('websites', 'on'): 4,\n",
       "         ('on', 'world'): 11,\n",
       "         ('world', 'wide'): 8,\n",
       "         ('wide', 'web'): 8,\n",
       "         ('did', 'sen'): 9,\n",
       "         ('sen', 'wayne'): 9,\n",
       "         ('wayne', 'morse'): 9,\n",
       "         ('morse', 'die'): 9,\n",
       "         ('a', 'vote'): 8,\n",
       "         ('vote', 'to'): 8,\n",
       "         ('to', 'table'): 8,\n",
       "         ('table', 'a'): 8,\n",
       "         ('a', 'motion'): 8,\n",
       "         ...})"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "bigram_list = []\n",
    "for index, row in train_data[0:].iterrows():\n",
    "    question = question_data.iloc[row['qIndex']]['question']\n",
    "    tokenized_question = [word.lower() for word in word_tokenize(question)]\n",
    "    bigram_list.extend(list(bigrams(tokenized_question)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
